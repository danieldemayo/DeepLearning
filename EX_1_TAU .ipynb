{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression with a Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your first programming assignment! \n",
    "In this assignment you will build your first classification and regression neural networks.\n",
    "This assignment is a 'step-through' guide to implement a simple fully-connected neural network in Pytorch.\n",
    "\n",
    "* In the first part of this exercise, you will implement a neural network with a 2 dimensional input. Your dataset is based on a two circles-shaped groups for classification.\n",
    "\n",
    "* Then, in the second part of this exercise, you will implement a regression model for predicting the output of a two dimensional function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "Let's first import all the packages that you will need during this part of assignment.\n",
    "\n",
    "Feel free to use another libraries if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import sklearn.metrics as metrics\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = 0\n",
    "np.random.seed(random_num)\n",
    "torch.manual_seed(random_num)\n",
    "x, y = make_circles(500, noise=0.075)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the dataset using matplotlib:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a 2-class classification neural network with a zero hidden layer.\n",
    "2. Plot loss vs epoch.\n",
    "3. Plot AUC vs epoch for train and test sets. \n",
    "4. Plot ROC curve and calculate AUC for the test set.\n",
    "5. Plot the learned decision boundary.\n",
    "6. Briefly interpret graph's results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you will do it:\n",
    "\n",
    "* Prepare the Data.\n",
    "* Define the Model.\n",
    "* Train the Model.\n",
    "* Evaluate the Model.\n",
    "* Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_inputs, ):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.lin1 = nn.Linear(num_inputs, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.lin1(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_to_tensor(x_train, x_test, y_train, y_test):\n",
    "    x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "    x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "    y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "    y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "    y_train = y_train.view(y_train.shape[0], 1)\n",
    "    y_test = y_test.view(y_test.shape[0], 1)\n",
    "    return x_train, x_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_INPUTS = 2\n",
    "LR = 1\n",
    "MOMENTUM = 0.9\n",
    "NUMBER_OF_EPOCHS = 1000\n",
    "\n",
    "model = LogisticRegression(num_inputs=NUM_INPUTS)\n",
    "optimizer = SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "criterion = nn.BCELoss()\n",
    "number_of_epochs = NUMBER_OF_EPOCHS\n",
    "x_train, x_test, y_train, y_test = data_to_tensor(x_train, x_test, y_train, y_test)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "preds = []\n",
    "rocs = []\n",
    "for epoch in range(number_of_epochs):\n",
    "    y_prediction = model(x_train)\n",
    "    loss = criterion(y_prediction, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    epochs.append(epoch)\n",
    "    losses.append(loss.item())\n",
    "    preds.append(y_prediction.detach().numpy())\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_train, preds[epoch])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    rocs.append(roc_auc)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('epoch:', epoch + 1, ',loss=', loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_prediction_test = model(x_test)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test.detach().numpy(), y_prediction_test.detach().numpy())\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, losses)\n",
    "# plt.figure(figsize=(16,8))\n",
    "plt.title('Epochs Vs. Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, rocs)\n",
    "plt.title('Epochs Vs. AUCs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, marker='.', label='ROC curve (area = %0.2f)', color='blue')\n",
    "plt.plot([0, 1], [0, 1], color='orange', lw=2, linestyle='--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "print(roc_auc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epoch\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_train.detach().numpy(), preds[epoch])\n",
    "plt.plot(fpr, tpr, marker='.', label='ROC curve (area = %0.2f)', color='blue')\n",
    "plt.plot([0, 1], [0, 1], color='orange', lw=2, linestyle='--')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_range = np.linspace(min(x[:, 0]), max(x[:, 0]))\n",
    "y_range = np.linspace(min(x[:, 1]), max(x[:, 1]))\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "grid = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "pred_func = model.forward(grid)\n",
    "z = pred_func.view(xx.shape).detach().numpy()\n",
    "z[z >= 0.5] = 1\n",
    "z[z < 0.5] = 0\n",
    "plt.contourf(xx, yy, z, cmap='RdBu')\n",
    "\n",
    "plt.ylabel('x2')\n",
    "plt.xlabel('x1')\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, cmap='Paired', s=6\n",
    "            )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Is the decision boundary linear or nonlinear in the case of a logistic regression? Explain.\n",
    "\n",
    "    The decision boundary is linear as we can see at the plot. M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you implemented \"classical\" logistic regression, now you will be implementing a neural network with one or more hidden layers.\n",
    "You will need to choose the number of hidden layers and nodes in a feedforward neural network, activation function, the type of optimizer and its hyperparmeters which will give you the best result. Remember, we don't want to overfit the training data, we want to generalize the solution for new data not seen during training. \n",
    "\n",
    "Plot the same graphs as in the previous sections and explain the similarities and differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLogisticRegression(nn.Module):\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        super(DeepLogisticRegression, self).__init__()\n",
    "        self.lin1 = nn.Linear(num_inputs, num_neurons)\n",
    "        self.lin2 = nn.Linear(num_neurons, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.lin1(x))\n",
    "        x = self.sig(self.lin2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 2\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "NUMBER_OF_EPOCHS = 1000\n",
    "NUM_NEURONS = 5\n",
    "\n",
    "model = DeepLogisticRegression(num_inputs=NUM_INPUTS, num_neurons=NUM_NEURONS)\n",
    "optimizer = SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "criterion = nn.BCELoss()\n",
    "number_of_epochs = NUMBER_OF_EPOCHS\n",
    "# x_train, x_test, y_train, y_test = data_to_tensor(x_train, x_test, y_train, y_test)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "preds = []\n",
    "rocs = []\n",
    "for epoch in range(number_of_epochs):\n",
    "    y_prediction = model(x_train)\n",
    "    loss = criterion(y_prediction, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    epochs.append(epoch)\n",
    "    losses.append(loss.item())\n",
    "    preds.append(y_prediction.detach().numpy())\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_train, preds[epoch])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    rocs.append(roc_auc)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('epoch:', epoch + 1, ',loss=', loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Why does a neural network need a non-linear activation function? try to figure out what heppan when you remove the activation function and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the excercise you will need to implement a regression model using neural networks. The model should predict the output of a trigonometric function of two variables. Your data set is based on a meshgrid. Your task is to create a list of points that would correspond to a grid and use it for the input of your neural network. Then, build your neural networks and find the architecture which gives you the best results.\n",
    "1. Plot the surface from the overall data and compare it to your predicted test sets.\n",
    "2. Which loss function and validation metric did you choose?\n",
    "3. Plot the loss and validation metrics vs epoch for the training and test sets.\n",
    "4. Build a new neural network and try overfitting your training set. Show the overfitting by using learning curve plots. \n",
    "    **Note**: You can use plt.ylim() function to better focus on the changes in the trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all the packages that you will need during this part of assignment.\n",
    "\n",
    "Feel free to use another libraries if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from torch import nn, from_numpy, Tensor, manual_seed, no_grad\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD, Optimizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_num)\n",
    "x = np.linspace(-5, 5, 30)\n",
    "y = np.linspace(-5, 5, 30)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = np.sin(xx) * np.cos(yy) + 0.1 * np.random.rand(xx.shape[0], xx.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Helper functions:**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid(scale_x: Tuple[int, int], scale_y: Tuple[int, int]) -> Tuple[NDArray, NDArray]:\n",
    "    x = np.linspace(*scale_x, 30)\n",
    "    y = np.linspace(*scale_y, 30)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def trigo_function(x1, x2):\n",
    "    return np.sin(x1) * np.cos(x2) + 0.1 * np.random.rand(x1.shape[0], x1.shape[1])\n",
    "\n",
    "\n",
    "def generate_data() -> Tuple[Tuple, NDArray]:\n",
    "    plane = grid((-5, 5), (-5, 5))\n",
    "    fx = trigo_function(*plane)\n",
    "    return plane, fx\n",
    "\n",
    "\n",
    "data = generate_data()\n",
    "\n",
    "\n",
    "def vectorize_data(x: NDArray, y: NDArray, z: NDArray) -> Tuple[NDArray, NDArray]:\n",
    "    xx = x.reshape(-1, 1)\n",
    "    yy = y.reshape(-1, 1)\n",
    "    input_data = np.hstack((xx, yy))\n",
    "    return input_data, z.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def convert_to_tensor(array: NDArray):\n",
    "    return from_numpy(array.astype(np.float32))\n",
    "\n",
    "\n",
    "def viz_data(x: NDArray, y: NDArray, z: NDArray, show: bool = True):\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x, y, z, color='b', alpha=0.5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.view_init(60, 35)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def viz_preds(data, preds):\n",
    "    x, fx = vectorize_data(*data[0], data[1])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, fx, test_size=0.3, random_state=1)\n",
    "    ax = viz_data(*data[0], data[1], show=False)\n",
    "    ax.scatter(x_test[:, 0], x_test[:, 1], preds, color='r')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "viz_data(*data[0], data[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_inputs: int, num_neurons: List):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(num_inputs, num_neurons[0])\n",
    "        self.lin2 = nn.Linear(num_neurons[0], num_neurons[1])\n",
    "        self.lin3 = nn.Linear(num_neurons[1], 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.relu(self.lin1(x))\n",
    "        x = self.sig(self.lin2(x))\n",
    "        x = self.sig(self.lin3(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "        model: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        loss_fn: nn.MSELoss,\n",
    "        x_train: Tensor, y_train: Tensor\n",
    ") -> Tensor:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_model(model: nn.Module, x_test: Tensor, y_test: Tensor, loss_fn: nn.MSELoss) -> Tuple[Tensor, Tensor]:\n",
    "    model.eval()\n",
    "    with no_grad():\n",
    "        y_pred = model(x_test)\n",
    "        loss = loss_fn(y_pred, y_test)\n",
    "    return loss, y_pred.ravel()\n",
    "\n",
    "\n",
    "def run_model(model: nn.Module, data: Tuple[Tuple, NDArray], num_of_epochs: int) -> [List, List, Dict]:\n",
    "    lr = 0.1\n",
    "    momentum = 0.9\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    x, fx = vectorize_data(*data[0], data[1])\n",
    "    x_train, x_test, y_train, y_test = (convert_to_tensor(v) for v in\n",
    "                                        train_test_split(x, fx, test_size=0.3, random_state=1))\n",
    "    pred = np.array([1])\n",
    "    for epoch in range(num_of_epochs):\n",
    "        train_loss = train_model(model, optimizer, loss_function, x_train, y_train)\n",
    "        test_loss, test_pred = test_model(model, x_test, y_test, loss_function)\n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        pred = test_pred.detach().numpy()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print('epoch:', epoch + 1, ',train_loss =', train_loss.item())\n",
    "            print('epoch:', epoch + 1, ',test_loss =', test_loss.item())\n",
    "        # validation usage\n",
    "    return train_losses, test_losses, dict(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, pred=pred)\n",
    "\n",
    "\n",
    "\n",
    "regression_model = RegressionModel(2, [3, 3])\n",
    "\n",
    "train_losses, test_losses, splited_data = run_model(model=regression_model, data=data, num_of_epochs=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_epochs(num_of_epochs: int,\n",
    "               other_axis: list,\n",
    "               plot_test: bool,\n",
    "               y_lim: tuple = (0.22, 0.28)\n",
    "               ):\n",
    "    epochs = list(range(num_of_epochs))\n",
    "    plt.plot(epochs, other_axis[0], 'orange', label='Train MSEs')\n",
    "    if plot_test:\n",
    "        plt.plot(epochs, other_axis[1], 'blue', label='Test MSEs', linestyle='--')\n",
    "    plt.ylim(list(y_lim))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "viz_epochs(num_of_epochs=1000, other_axis=[train_losses, test_losses], plot_test=True, )\n",
    "\n",
    "viz_preds(data, splited_data['pred'], )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a new neural network and try overfitting your training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_num)\n",
    "x = np.linspace(-5, 5, 30)\n",
    "y = np.linspace(-5, 5, 30)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = np.sin(xx) * np.cos(yy) + 0.1 * np.random.rand(xx.shape[0], xx.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfitModel(nn.Module):\n",
    "    def __init__(self, num_inputs: int, num_neurons: list):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(num_inputs, num_neurons[0])\n",
    "        self.lin2 = nn.Linear(num_neurons[0], num_neurons[1])\n",
    "        self.lin3 = nn.Linear(num_neurons[1], num_neurons[2])\n",
    "        self.lin4 = nn.Linear(num_neurons[2], 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.lin1(x))\n",
    "        x = self.sig(self.lin2(x))\n",
    "        x = self.sig(self.lin3(x))\n",
    "        x = self.sig(self.lin4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_model = OverfitModel(2, [5, 5, 5])\n",
    "\n",
    "train_losses, test_losses, splited_data = run_model(model=regression_model, data=data, num_of_epochs=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_epochs(num_of_epochs=10000, other_axis=[train_losses, test_losses], plot_test=True, )\n",
    "\n",
    "viz_preds(data, splited_data['pred'], )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Briefly explain graph's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How does your metric value differs between the training data and the test data and why?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}